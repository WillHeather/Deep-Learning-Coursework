{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Kaggle Competition: Footprint Image Classification\n",
    "\n",
    "- **Student Name:** [TO FILL]\n",
    "- **Student ID:** [TO FILL]\n",
    "- **Kaggle Username:** [TO FILL]\n",
    "- **Final Private Leaderboard Score:** [TO FILL]\n",
    "- **Total Number of Submissions:** [TO FILL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "**Problem:** Sex classification from footprint images for forensic analysis\n",
    "\n",
    "**Why Deep Learning:** CNNs automatically learn features, traditional ML requires manual engineering\n",
    "\n",
    "**Objectives:** Baseline → experiments → SOTA → explainability → Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: EDA & Preprocessing\n",
    "\n",
    "This section covers:\n",
    "- Setup and imports\n",
    "- Data loading and exploration\n",
    "- Dataset statistics and visualization\n",
    "- Data preprocessing and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & Imports\n",
    "!pip install timm optuna -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import timm\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Device selection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Initialize global variables (will be updated based on actual data)\n",
    "INPUT_CHANNELS = 3  # Default to RGB\n",
    "IS_GRAYSCALE = False\n",
    "BATCH_SIZE = 128  # Balanced: good speed, safer than 512\n",
    "\n",
    "print('Setup complete!')\n",
    "print('\\nNow run ONE of the following cells to set data paths:')\n",
    "print('  - OPTION A: Local data (next cell)')\n",
    "print('  - OPTION B: Google Drive data (cell after that)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION A: Local Data (Run this cell if running locally)\n",
    "# =============================================================================\n",
    "# Data paths for local execution\n",
    "DATA_DIR = Path('./data')\n",
    "TRAIN_DIR = Path('./data/train')\n",
    "TEST_DIR = Path('./data/test')\n",
    "\n",
    "print('Using LOCAL data paths:')\n",
    "print(f'  DATA_DIR: {DATA_DIR}')\n",
    "print(f'  TRAIN_DIR: {TRAIN_DIR}')\n",
    "print(f'  TEST_DIR: {TEST_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION B: Google Drive Data (Run this cell if running in Google Colab)\n",
    "# =============================================================================\n",
    "# Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Data paths for Google Colab - adjust if your folder structure differs\n",
    "# DATA_DIR = Path('/content/drive/MyDrive/Colab Files/DeepLearningData')\n",
    "# TRAIN_DIR = DATA_DIR / 'train'\n",
    "# TEST_DIR = DATA_DIR / 'test'\n",
    "\n",
    "# print('Using GOOGLE DRIVE data paths:')\n",
    "# print(f'  DATA_DIR: {DATA_DIR}')\n",
    "# print(f'  TRAIN_DIR: {TRAIN_DIR}')\n",
    "# print(f'  TEST_DIR: {TEST_DIR}')\n",
    "\n",
    "# # Verify paths exist\n",
    "# if DATA_DIR.exists():\n",
    "#     print('\\nData directory found!')\n",
    "#     print(f'Contents: {list(DATA_DIR.iterdir())}')\n",
    "# else:\n",
    "#     print(f'\\nWARNING: Data directory not found at {DATA_DIR}')\n",
    "    # print('Please check your Google Drive path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load image paths and labels from directory structure\"\"\"\n",
    "    paths, labels = [], []\n",
    "    for lbl in [0, 1]:\n",
    "        folder = data_dir / str(lbl)\n",
    "        if folder.exists():\n",
    "            for ext in ['*.jpg', '*.png']:\n",
    "                for p in folder.glob(ext):\n",
    "                    paths.append(str(p))\n",
    "                    labels.append(lbl)\n",
    "    return paths, labels\n",
    "\n",
    "# Load training data\n",
    "if TRAIN_DIR.exists():\n",
    "    train_paths, train_labels = load_data(TRAIN_DIR)\n",
    "    train_df = pd.DataFrame({'path': train_paths, 'label': train_labels})\n",
    "    print(f'Loaded {len(train_df)} training images')\n",
    "    print('\\nClass distribution:')\n",
    "    print(train_df['label'].value_counts().sort_index())\n",
    "else:\n",
    "    train_df = pd.DataFrame()\n",
    "    print('Training data directory not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Class Distribution\n",
    "if len(train_df) > 0:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    train_df['label'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Class (0=Female, 1=Male)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Image Properties\n",
    "if len(train_df) > 0:\n",
    "    # Check first image to determine if grayscale or RGB\n",
    "    sample_img = Image.open(train_df.iloc[0]['path'])\n",
    "    IS_GRAYSCALE = sample_img.mode == 'L'\n",
    "    INPUT_CHANNELS = 1 if IS_GRAYSCALE else 3\n",
    "    \n",
    "    print(f'Image mode: {sample_img.mode}')\n",
    "    print(f'Input channels: {INPUT_CHANNELS} ({\"Grayscale\" if IS_GRAYSCALE else \"RGB\"})')\n",
    "    print(f'Image size: {sample_img.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transforms\n",
    "if len(train_df) > 0:\n",
    "    # Normalization parameters\n",
    "    mean_std = ([0.5], [0.5]) if IS_GRAYSCALE else ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "    # Training transforms with STRONG augmentation for better generalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize larger for random crop\n",
    "        transforms.RandomCrop(224),      # Random crop for position invariance\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # Left/right flip\n",
    "        transforms.RandomRotation(15),   # Rotation ±15 degrees\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,  # Random brightness adjustment\n",
    "            contrast=0.2,    # Random contrast adjustment\n",
    "            saturation=0.1,  # Random saturation adjustment\n",
    "            hue=0.05         # Slight hue shift\n",
    "        ),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0,              # No additional rotation\n",
    "            translate=(0.1, 0.1),   # Shift up to 10% in x/y\n",
    "            scale=(0.9, 1.1),       # Scale 90% to 110%\n",
    "            shear=5                 # Shear up to 5 degrees\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*mean_std),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))  # Random patch erasing\n",
    "    ])\n",
    "\n",
    "    # Validation/test transforms (no augmentation - deterministic)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*mean_std)\n",
    "    ])\n",
    "\n",
    "    print('Transforms defined with strong augmentation:')\n",
    "    print('  - RandomCrop (256->224)')\n",
    "    print('  - RandomHorizontalFlip')\n",
    "    print('  - RandomRotation (±15°)')\n",
    "    print('  - ColorJitter (brightness, contrast, saturation, hue)')\n",
    "    print('  - RandomAffine (translate, scale, shear)')\n",
    "    print('  - RandomErasing (20% probability)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset Class and DataLoaders\n",
    "class FootprintDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('L' if IS_GRAYSCALE else 'RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "if len(train_df) > 0:\n",
    "    # Split into train and validation sets (80/20)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_df['path'].tolist(),\n",
    "        train_df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=train_df['label']\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FootprintDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = FootprintDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    # Create dataloaders (num_workers=0 for Windows compatibility, but larger batch size for better GPU utilization)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0,  # Windows-safe: single-threaded loading\n",
    "        pin_memory=True  # Faster GPU transfer\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0,  # Windows-safe: single-threaded loading\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f'Train samples: {len(train_dataset)}')\n",
    "    print(f'Validation samples: {len(val_dataset)}')\n",
    "    print(f'Batch size: {BATCH_SIZE}')\n",
    "    print(f'Note: num_workers=0 for Windows compatibility, batch_size={BATCH_SIZE} for better GPU utilization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Baseline Model\n",
    "\n",
    "This section implements a simple CNN from scratch to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Baseline CNN Architecture\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, input_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print('Baseline CNN architecture defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(loader, desc='Training', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc='Evaluating', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "print('Training functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Training Loop Function\n",
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"\n",
    "    Generic training function that works with any model and config\n",
    "    \n",
    "    config should contain: epochs, lr, optimizer ('sgd' or 'adam'), weight_decay (optional)\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create optimizer\n",
    "    if config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], \n",
    "                             momentum=0.9, weight_decay=config.get('weight_decay', 0))\n",
    "    else:  # adam\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'], \n",
    "                              weight_decay=config.get('weight_decay', 0))\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f'\\nBest validation accuracy: {best_val_acc:.4f}')\n",
    "    \n",
    "    return model, history, best_val_acc\n",
    "\n",
    "print('Generic training loop defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline Model\n",
    "if len(train_df) > 0:\n",
    "    # Configuration for baseline\n",
    "    baseline_config = {\n",
    "        'epochs': 10,\n",
    "        'lr': 0.001,\n",
    "        'optimizer': 'sgd'\n",
    "    }\n",
    "    \n",
    "    # Create and train model\n",
    "    baseline_model = BaselineCNN(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    baseline_model, baseline_hist, baseline_acc = train_model(\n",
    "        baseline_model, train_loader, val_loader, baseline_config, device\n",
    "    )\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    experiment_results = [{\n",
    "        'name': 'Baseline',\n",
    "        'val_accuracy': baseline_acc,\n",
    "        'val_loss': baseline_hist['val_loss'][-1]\n",
    "    }]\n",
    "    \n",
    "    print(f'\\nBaseline model training complete!')\n",
    "    print(f'Final validation accuracy: {baseline_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Learning Curves\n",
    "if len(train_df) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(baseline_hist['train_loss'], 'o-', label='Train')\n",
    "    ax1.plot(baseline_hist['val_loss'], 's-', label='Validation')\n",
    "    ax1.set_title('Baseline Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(baseline_hist['train_acc'], 'o-', label='Train')\n",
    "    ax2.plot(baseline_hist['val_acc'], 's-', label='Validation')\n",
    "    ax2.set_title('Baseline Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: State-of-the-Art Model Analysis\n",
    "\n",
    "This section analyzes three SOTA architectures for potential use in the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count parameters\n",
    "def count_params(model):\n",
    "    \"\"\"Count trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Parameter counting function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: ResNet-18\n",
    "print('='*60)\n",
    "print('1. ResNet-18')\n",
    "print('='*60)\n",
    "\n",
    "r18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify first conv layer if grayscale\n",
    "if INPUT_CHANNELS == 1:\n",
    "    r18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Replace final layer for binary classification\n",
    "r18.fc = nn.Linear(r18.fc.in_features, 2)\n",
    "\n",
    "print(f'Parameters: {count_params(r18):,}')\n",
    "print(f'Input channels: {INPUT_CHANNELS}')\n",
    "print(f'Output classes: 2')\n",
    "print('\\nKey features:')\n",
    "print('- Residual connections (skip connections)')\n",
    "print('- Deep architecture (18 layers)')\n",
    "print('- Pre-trained on ImageNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: EfficientNet-B0\n",
    "print('='*60)\n",
    "print('2. EfficientNet-B0')\n",
    "print('='*60)\n",
    "\n",
    "eff = timm.create_model('efficientnet_b0', pretrained=True, num_classes=2, in_chans=INPUT_CHANNELS)\n",
    "\n",
    "print(f'Parameters: {count_params(eff):,}')\n",
    "print(f'Input channels: {INPUT_CHANNELS}')\n",
    "print(f'Output classes: 2')\n",
    "print('\\nKey features:')\n",
    "print('- Compound scaling (depth, width, resolution)')\n",
    "print('- Mobile inverted bottleneck convolutions')\n",
    "print('- Efficient architecture, fewer params than ResNet')\n",
    "print('- Pre-trained on ImageNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Vision Transformer (ViT)\n",
    "print('='*60)\n",
    "print('3. Vision Transformer (ViT-Base/16)')\n",
    "print('='*60)\n",
    "\n",
    "vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2, in_chans=INPUT_CHANNELS)\n",
    "\n",
    "print(f'Parameters: {count_params(vit):,}')\n",
    "print(f'Input channels: {INPUT_CHANNELS}')\n",
    "print(f'Output classes: 2')\n",
    "print('\\nKey features:')\n",
    "print('- Transformer architecture (attention-based)')\n",
    "print('- Patch-based processing (16x16 patches)')\n",
    "print('- No convolutions - pure attention')\n",
    "print('- Pre-trained on ImageNet')\n",
    "print('- May require more data to train effectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Systematic Experiments\n",
    "\n",
    "This section runs 10 different experiments to improve upon the baseline model systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional model architectures for experiments\n",
    "class CNN_BatchNorm(nn.Module):\n",
    "    \"\"\"CNN with Batch Normalization\"\"\"\n",
    "    def __init__(self, num_classes=2, input_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_Dropout(nn.Module):\n",
    "    \"\"\"CNN with Dropout regularization\"\"\"\n",
    "    def __init__(self, num_classes=2, input_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print('Additional model architectures defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Experiment Tracking Variables\n",
    "# Run this cell BEFORE any experiments to ensure all shared variables are defined\n",
    "\n",
    "if len(train_df) > 0:\n",
    "    # Initialize experiment results list (if not already done by baseline)\n",
    "    if 'experiment_results' not in dir() or not isinstance(experiment_results, list):\n",
    "        experiment_results = []\n",
    "        print('Initialized experiment_results list')\n",
    "    \n",
    "    # Initialize model storage dictionary\n",
    "    if 'all_models' not in dir() or not isinstance(all_models, dict):\n",
    "        all_models = {}\n",
    "        print('Initialized all_models dictionary')\n",
    "    \n",
    "    # Learning rate scaling factor for batch size\n",
    "    LR_SCALE = BATCH_SIZE / 32\n",
    "    print(f'Set LR_SCALE = {LR_SCALE} (for BATCH_SIZE={BATCH_SIZE})')\n",
    "    \n",
    "    print('\\nExperiment tracking variables ready!')\n",
    "    print(f'  - experiment_results: {len(experiment_results)} entries')\n",
    "    print(f'  - all_models: {len(all_models)} models')\n",
    "    print(f'  - LR_SCALE: {LR_SCALE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Adam Optimizer\n",
    "\n",
    "**Hypothesis:** Adam optimizer will converge faster than SGD\n",
    "\n",
    "**Change:** Replace SGD with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Adam Optimizer\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 1: Adam Optimizer')\n",
    "    print('='*60)\n",
    "    \n",
    "    exp1_config = {\n",
    "        'epochs': 10,\n",
    "        'lr': 0.001,\n",
    "        'optimizer': 'adam'\n",
    "    }\n",
    "    \n",
    "    exp1_model = BaselineCNN(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    exp1_model, exp1_hist, exp1_acc = train_model(exp1_model, train_loader, val_loader, exp1_config, device)\n",
    "    \n",
    "    experiment_results.append({\n",
    "        'name': 'Exp1: Adam',\n",
    "        'val_accuracy': exp1_acc,\n",
    "        'val_loss': exp1_hist['val_loss'][-1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Batch Normalization\n",
    "\n",
    "**Hypothesis:** Batch normalization will stabilize training and improve accuracy\n",
    "\n",
    "**Change:** Add BatchNorm2d layers after each convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Batch Normalization\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 2: Batch Normalization')\n",
    "    print('='*60)\n",
    "    \n",
    "    exp2_config = {\n",
    "        'epochs': 10,\n",
    "        'lr': 0.001,\n",
    "        'optimizer': 'sgd'\n",
    "    }\n",
    "    \n",
    "    exp2_model = CNN_BatchNorm(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    exp2_model, exp2_hist, exp2_acc = train_model(exp2_model, train_loader, val_loader, exp2_config, device)\n",
    "    \n",
    "    experiment_results.append({\n",
    "        'name': 'Exp2: BatchNorm',\n",
    "        'val_accuracy': exp2_acc,\n",
    "        'val_loss': exp2_hist['val_loss'][-1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Dropout Regularization\n",
    "\n",
    "**Hypothesis:** Dropout will reduce overfitting by randomly dropping neurons during training\n",
    "\n",
    "**Change:** Add dropout layer (50%) in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Dropout Regularization\n",
    "if len(train_df) > 0:\n",
    "    # Initialize shared variables for experiments 3-10\n",
    "    all_models = {}\n",
    "    LR_SCALE = BATCH_SIZE / 32  # Scale LR for batch size\n",
    "    \n",
    "    print('='*60)\n",
    "    print('Experiment 3: Dropout Regularization')\n",
    "    print('='*60)\n",
    "    exp3_config = {'epochs': 10, 'lr': 0.001 * LR_SCALE, 'optimizer': 'sgd'}\n",
    "    exp3_model = CNN_Dropout(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    exp3_model, exp3_hist, exp3_acc = train_model(exp3_model, train_loader, val_loader, exp3_config, device)\n",
    "    experiment_results.append({'name': 'Exp3: Dropout', 'val_accuracy': exp3_acc, 'val_loss': exp3_hist['val_loss'][-1]})\n",
    "    all_models['Exp3: Dropout'] = exp3_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Weight Decay (L2 Regularization)\n",
    "\n",
    "**Hypothesis:** L2 regularization will prevent overfitting by penalizing large weights\n",
    "\n",
    "**Change:** Add weight decay (1e-4) to the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Weight Decay\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 4: Weight Decay (L2 Regularization)')\n",
    "    print('='*60)\n",
    "    exp4_config = {'epochs': 10, 'lr': 0.001 * LR_SCALE, 'optimizer': 'sgd', 'weight_decay': 1e-4}\n",
    "    exp4_model = CNN_Dropout(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    exp4_model, exp4_hist, exp4_acc = train_model(exp4_model, train_loader, val_loader, exp4_config, device)\n",
    "    experiment_results.append({'name': 'Exp4: WeightDecay', 'val_accuracy': exp4_acc, 'val_loss': exp4_hist['val_loss'][-1]})\n",
    "    all_models['Exp4: WeightDecay'] = exp4_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Adam + Dropout + Weight Decay\n",
    "\n",
    "**Hypothesis:** Combining Adam optimizer with regularization techniques will yield better results\n",
    "\n",
    "**Change:** Use Adam optimizer with dropout and weight decay together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Adam + Dropout + Weight Decay\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 5: Adam + Dropout + Weight Decay')\n",
    "    print('='*60)\n",
    "    exp5_config = {'epochs': 10, 'lr': 0.001 * LR_SCALE, 'optimizer': 'adam', 'weight_decay': 1e-4}\n",
    "    exp5_model = CNN_Dropout(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    exp5_model, exp5_hist, exp5_acc = train_model(exp5_model, train_loader, val_loader, exp5_config, device)\n",
    "    experiment_results.append({'name': 'Exp5: Adam+Drop+WD', 'val_accuracy': exp5_acc, 'val_loss': exp5_hist['val_loss'][-1]})\n",
    "    all_models['Exp5: Adam+Drop+WD'] = exp5_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: ResNet-18 (Frozen Features)\n",
    "\n",
    "**Hypothesis:** Pre-trained ImageNet features will work well for footprint classification\n",
    "\n",
    "**Change:** Use ResNet-18 with frozen convolutional layers, only train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: ResNet-18 Frozen\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 6: ResNet-18 (Frozen Features)')\n",
    "    print('='*60)\n",
    "    exp6_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    if INPUT_CHANNELS == 1:\n",
    "        exp6_model.conv1 = nn.Conv2d(1, 64, 7, 2, 3, bias=False)\n",
    "    for param in exp6_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    exp6_model.fc = nn.Linear(exp6_model.fc.in_features, 2)\n",
    "    exp6_model = exp6_model.to(device)\n",
    "    exp6_config = {'epochs': 10, 'lr': 0.001 * LR_SCALE, 'optimizer': 'adam'}\n",
    "    exp6_model, exp6_hist, exp6_acc = train_model(exp6_model, train_loader, val_loader, exp6_config, device)\n",
    "    experiment_results.append({'name': 'Exp6: ResNet Frozen', 'val_accuracy': exp6_acc, 'val_loss': exp6_hist['val_loss'][-1]})\n",
    "    all_models['Exp6: ResNet Frozen'] = exp6_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: ResNet-18 (Fine-tuned)\n",
    "\n",
    "**Hypothesis:** Fine-tuning all layers will allow the model to adapt to footprint-specific features\n",
    "\n",
    "**Change:** Train all ResNet-18 layers with a smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7: ResNet-18 Fine-tuned (with Optuna-optimized hyperparameters)\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 7: ResNet-18 (Fine-tuned with Best Hyperparameters)')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Optimized hyperparameters from Optuna\n",
    "    best_lr = 0.000460\n",
    "    best_weight_decay = 0.000417\n",
    "    best_dropout = 0.199358\n",
    "    best_batch_size = 64\n",
    "    best_epochs = 16\n",
    "    best_rotation = 13\n",
    "    best_color_jitter = 0.122036\n",
    "    \n",
    "    print(f'Using optimized hyperparameters:')\n",
    "    print(f'  lr: {best_lr}')\n",
    "    print(f'  weight_decay: {best_weight_decay}')\n",
    "    print(f'  dropout: {best_dropout:.2f}')\n",
    "    print(f'  batch_size: {best_batch_size}')\n",
    "    print(f'  epochs: {best_epochs}')\n",
    "    print(f'  rotation: {best_rotation}')\n",
    "    print(f'  color_jitter: {best_color_jitter}')\n",
    "    \n",
    "    # Create optimized transforms\n",
    "    train_transform_exp7 = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(best_rotation),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=best_color_jitter,\n",
    "            contrast=best_color_jitter,\n",
    "            saturation=best_color_jitter * 0.5\n",
    "        ),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*mean_std),\n",
    "        transforms.RandomErasing(p=0.2)\n",
    "    ])\n",
    "    \n",
    "    # Create datasets and loaders with optimized batch size\n",
    "    train_dataset_exp7 = FootprintDataset(train_paths, train_labels, train_transform_exp7)\n",
    "    train_loader_exp7 = DataLoader(\n",
    "        train_dataset_exp7, \n",
    "        batch_size=best_batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader_exp7 = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=best_batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model with optimized dropout\n",
    "    exp7_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    if INPUT_CHANNELS == 1:\n",
    "        exp7_model.conv1 = nn.Conv2d(1, 64, 7, 2, 3, bias=False)\n",
    "    num_features = exp7_model.fc.in_features\n",
    "    exp7_model.fc = nn.Sequential(\n",
    "        nn.Dropout(best_dropout),\n",
    "        nn.Linear(num_features, 2)\n",
    "    )\n",
    "    exp7_model = exp7_model.to(device)\n",
    "    \n",
    "    # Train with optimized config\n",
    "    exp7_config = {\n",
    "        'epochs': best_epochs, \n",
    "        'lr': best_lr, \n",
    "        'optimizer': 'adam', \n",
    "        'weight_decay': best_weight_decay\n",
    "    }\n",
    "    exp7_model, exp7_hist, exp7_acc = train_model(\n",
    "        exp7_model, train_loader_exp7, val_loader_exp7, exp7_config, device\n",
    "    )\n",
    "    \n",
    "    experiment_results.append({\n",
    "        'name': 'Exp7: ResNet Finetuned', \n",
    "        'val_accuracy': exp7_acc, \n",
    "        'val_loss': exp7_hist['val_loss'][-1]\n",
    "    })\n",
    "    all_models['Exp7: ResNet Finetuned'] = exp7_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8: EfficientNet-B0\n",
    "\n",
    "**Hypothesis:** EfficientNet's compound scaling will provide better accuracy with fewer parameters\n",
    "\n",
    "**Change:** Use pre-trained EfficientNet-B0 instead of ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 8: EfficientNet-B0\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 8: EfficientNet-B0')\n",
    "    print('='*60)\n",
    "    exp8_model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=2, in_chans=INPUT_CHANNELS).to(device)\n",
    "    exp8_config = {'epochs': 10, 'lr': 0.0001 * LR_SCALE, 'optimizer': 'adam', 'weight_decay': 1e-4}\n",
    "    exp8_model, exp8_hist, exp8_acc = train_model(exp8_model, train_loader, val_loader, exp8_config, device)\n",
    "    experiment_results.append({'name': 'Exp8: EfficientNet', 'val_accuracy': exp8_acc, 'val_loss': exp8_hist['val_loss'][-1]})\n",
    "    all_models['Exp8: EfficientNet'] = exp8_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9: Higher Learning Rate\n",
    "\n",
    "**Hypothesis:** A higher learning rate may speed up convergence and escape local minima\n",
    "\n",
    "**Change:** Increase learning rate from 0.001 to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 9: Higher Learning Rate\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 9: Higher Learning Rate (0.01)')\n",
    "    print('='*60)\n",
    "    exp9_config = {'epochs': 10, 'lr': 0.01 * LR_SCALE, 'optimizer': 'sgd'}\n",
    "    exp9_model = CNN_Dropout(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    exp9_model, exp9_hist, exp9_acc = train_model(exp9_model, train_loader, val_loader, exp9_config, device)\n",
    "    experiment_results.append({'name': 'Exp9: HighLR', 'val_accuracy': exp9_acc, 'val_loss': exp9_hist['val_loss'][-1]})\n",
    "    all_models['Exp9: HighLR'] = exp9_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 10: Longer Training\n",
    "\n",
    "**Hypothesis:** More training epochs will allow the model to better learn the data patterns\n",
    "\n",
    "**Change:** Train for 20 epochs instead of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 10: Longer Training\n",
    "if len(train_df) > 0:\n",
    "    print('='*60)\n",
    "    print('Experiment 10: Longer Training (20 epochs)')\n",
    "    print('='*60)\n",
    "    exp10_config = {'epochs': 20, 'lr': 0.001 * LR_SCALE, 'optimizer': 'adam'}\n",
    "    exp10_model = CNN_Dropout(num_classes=2, input_channels=INPUT_CHANNELS).to(device)\n",
    "    exp10_model, exp10_hist, exp10_acc = train_model(exp10_model, train_loader, val_loader, exp10_config, device)\n",
    "    experiment_results.append({'name': 'Exp10: LongTrain', 'val_accuracy': exp10_acc, 'val_loss': exp10_hist['val_loss'][-1]})\n",
    "    all_models['Exp10: LongTrain'] = exp10_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Best Model from All Experiments\n",
    "if len(train_df) > 0:\n",
    "    print('='*80)\n",
    "    print('SELECTING BEST MODEL')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Find the best experiment\n",
    "    best_experiment = max(experiment_results, key=lambda x: x['val_accuracy'])\n",
    "    best_model_name = best_experiment['name']\n",
    "    \n",
    "    # Set final_model based on best result\n",
    "    if best_model_name in all_models:\n",
    "        final_model = all_models[best_model_name]\n",
    "    elif 'exp8_model' in dir() and best_model_name == 'Exp8: EfficientNet':\n",
    "        final_model = exp8_model\n",
    "    elif 'exp7_model' in dir() and best_model_name == 'Exp7: ResNet Finetuned':\n",
    "        final_model = exp7_model\n",
    "    elif 'exp2_model' in dir() and best_model_name == 'Exp2: BatchNorm':\n",
    "        final_model = exp2_model\n",
    "    elif 'exp1_model' in dir() and best_model_name == 'Exp1: Adam':\n",
    "        final_model = exp1_model\n",
    "    elif 'baseline_model' in dir() and best_model_name == 'Baseline':\n",
    "        final_model = baseline_model\n",
    "    else:\n",
    "        # Fallback to most recent model in all_models\n",
    "        final_model = list(all_models.values())[-1] if all_models else exp10_model\n",
    "        print(f'Note: Could not find {best_model_name}, using fallback model')\n",
    "    \n",
    "    print(f'\\nBest Model: {best_model_name}')\n",
    "    print(f'Validation Accuracy: {best_experiment[\"val_accuracy\"]:.4f}')\n",
    "    print(f'Validation Loss: {best_experiment[\"val_loss\"]:.4f}')\n",
    "    print('\\nfinal_model is now set and ready for evaluation and submission.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Experiment Results Summary\n",
    "if len(train_df) > 0:\n",
    "    results_df = pd.DataFrame(experiment_results).sort_values('val_accuracy', ascending=False)\n",
    "    print('\\n' + '='*80)\n",
    "    print('EXPERIMENT RESULTS SUMMARY (Sorted by Validation Accuracy)')\n",
    "    print('='*80)\n",
    "    display(results_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    results_df_sorted = results_df.sort_values('val_accuracy')\n",
    "    ax.barh(results_df_sorted['name'], results_df_sorted['val_accuracy'])\n",
    "    ax.set_xlabel('Validation Accuracy')\n",
    "    ax.set_title('Experiment Comparison')\n",
    "    ax.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Final Model with Best Hyperparameters from Optuna\n",
    "if len(train_df) > 0 and 'study' in dir():\n",
    "    print('='*80)\n",
    "    print('TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_trial.params\n",
    "    \n",
    "    print('\\nUsing best hyperparameters:')\n",
    "    for key, value in best_params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Create optimized transforms\n",
    "    train_transform_final = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(best_params['rotation']),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=best_params['color_jitter'],\n",
    "            contrast=best_params['color_jitter'],\n",
    "            saturation=best_params['color_jitter'] * 0.5\n",
    "        ),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*mean_std),\n",
    "        transforms.RandomErasing(p=0.2)\n",
    "    ])\n",
    "    \n",
    "    # Create datasets and loaders with best batch size\n",
    "    train_dataset_final = FootprintDataset(train_paths, train_labels, train_transform_final)\n",
    "    train_loader_final = DataLoader(\n",
    "        train_dataset_final, \n",
    "        batch_size=best_params['batch_size'], \n",
    "        shuffle=True, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader_final = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=best_params['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model with best dropout\n",
    "    final_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    num_features = final_model.fc.in_features\n",
    "    final_model.fc = nn.Sequential(\n",
    "        nn.Dropout(best_params['dropout']),\n",
    "        nn.Linear(num_features, 2)\n",
    "    )\n",
    "    final_model = final_model.to(device)\n",
    "    \n",
    "    # Create optimizer with best parameters\n",
    "    if best_params['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'], \n",
    "                               weight_decay=best_params['weight_decay'])\n",
    "    elif best_params['optimizer'] == 'adamw':\n",
    "        optimizer = optim.AdamW(final_model.parameters(), lr=best_params['lr'], \n",
    "                                weight_decay=best_params['weight_decay'])\n",
    "    else:\n",
    "        optimizer = optim.SGD(final_model.parameters(), lr=best_params['lr'], \n",
    "                              momentum=0.9, weight_decay=best_params['weight_decay'])\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=best_params['epochs'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop with progress tracking\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(final_model.state_dict())\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"\\nTraining for {best_params['epochs']} epochs...\")\n",
    "    \n",
    "    for epoch in range(best_params['epochs']):\n",
    "        # Train\n",
    "        final_model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader_final, desc=f'Epoch {epoch+1}/{best_params[\"epochs\"]}', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = final_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validate\n",
    "        final_model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader_final:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = final_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        val_loss = val_loss / total\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(final_model.state_dict())\n",
    "    \n",
    "    # Load best weights\n",
    "    final_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print(f'\\n' + '='*80)\n",
    "    print(f'FINAL MODEL TRAINING COMPLETE')\n",
    "    print(f'='*80)\n",
    "    print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n",
    "    print(f'\\nThis model will be used for Kaggle submission.')\n",
    "    \n",
    "    # Update experiment results\n",
    "    experiment_results.append({\n",
    "        'name': 'Optuna Optimized',\n",
    "        'val_accuracy': best_val_acc,\n",
    "        'val_loss': history['val_loss'][-1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Optimization Results\n",
    "if len(train_df) > 0 and 'study' in dir():\n",
    "    print('='*80)\n",
    "    print('OPTIMIZATION VISUALIZATION')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Plot optimization history\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Optimization history\n",
    "    ax = axes[0, 0]\n",
    "    trials = [t for t in study.trials if t.state == TrialState.COMPLETE]\n",
    "    values = [t.value for t in trials]\n",
    "    ax.plot(range(len(values)), values, 'o-', alpha=0.7)\n",
    "    ax.axhline(y=max(values), color='r', linestyle='--', label=f'Best: {max(values):.4f}')\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('Validation Accuracy')\n",
    "    ax.set_title('Optimization History')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 2. Hyperparameter importance (if enough trials)\n",
    "    ax = axes[0, 1]\n",
    "    if len(trials) >= 5:\n",
    "        importances = optuna.importance.get_param_importances(study)\n",
    "        params = list(importances.keys())[:8]  # Top 8\n",
    "        values = [importances[p] for p in params]\n",
    "        ax.barh(params, values)\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_title('Hyperparameter Importance')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Need more trials\\nfor importance analysis', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Hyperparameter Importance')\n",
    "    \n",
    "    # 3. Learning rate vs accuracy\n",
    "    ax = axes[1, 0]\n",
    "    lrs = [t.params['lr'] for t in trials]\n",
    "    accs = [t.value for t in trials]\n",
    "    ax.scatter(lrs, accs, alpha=0.7)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning Rate (log scale)')\n",
    "    ax.set_ylabel('Validation Accuracy')\n",
    "    ax.set_title('Learning Rate vs Accuracy')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 4. Weight decay vs accuracy\n",
    "    ax = axes[1, 1]\n",
    "    wds = [t.params['weight_decay'] for t in trials]\n",
    "    ax.scatter(wds, accs, alpha=0.7)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Weight Decay (log scale)')\n",
    "    ax.set_ylabel('Validation Accuracy')\n",
    "    ax.set_title('Weight Decay vs Accuracy')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print trial summary table\n",
    "    print('\\nTop 5 Trials:')\n",
    "    trials_sorted = sorted(trials, key=lambda t: t.value, reverse=True)[:5]\n",
    "    for i, t in enumerate(trials_sorted, 1):\n",
    "        print(f\"\\n{i}. Accuracy: {t.value:.4f}\")\n",
    "        print(f\"   lr={t.params['lr']:.2e}, wd={t.params['weight_decay']:.2e}, \"\n",
    "              f\"opt={t.params['optimizer']}, bs={t.params['batch_size']}, \"\n",
    "              f\"epochs={t.params['epochs']}, dropout={t.params['dropout']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Hyperparameter Optimization for ResNet-18\n",
    "if len(train_df) > 0:\n",
    "    \n",
    "    def create_objective(train_paths, train_labels, val_dataset, device, mean_std):\n",
    "        \"\"\"\n",
    "        Factory function to create objective with access to data.\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            \"\"\"\n",
    "            Optuna objective function - trains one model with suggested hyperparameters\n",
    "            and returns validation accuracy.\n",
    "            \"\"\"\n",
    "            \n",
    "            # =================================================================\n",
    "            # HYPERPARAMETERS TO TUNE\n",
    "            # =================================================================\n",
    "            \n",
    "            # Learning rate (log scale: 1e-5 to 1e-2)\n",
    "            lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "            \n",
    "            # Weight decay (log scale: 1e-6 to 1e-2)\n",
    "            weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "            \n",
    "            # Optimizer choice\n",
    "            optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'adamw'])\n",
    "            \n",
    "            # Batch size\n",
    "            batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "            \n",
    "            # Number of epochs\n",
    "            epochs = trial.suggest_int('epochs', 10, 25)\n",
    "            \n",
    "            # Dropout in classifier\n",
    "            dropout_rate = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "            \n",
    "            # Augmentation parameters\n",
    "            rotation_degrees = trial.suggest_int('rotation', 5, 20)\n",
    "            color_jitter_strength = trial.suggest_float('color_jitter', 0.0, 0.3)\n",
    "            \n",
    "            # =================================================================\n",
    "            # CREATE DATA LOADERS WITH TUNED AUGMENTATION\n",
    "            # =================================================================\n",
    "            \n",
    "            train_transform_tuned = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(rotation_degrees),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=color_jitter_strength,\n",
    "                    contrast=color_jitter_strength,\n",
    "                    saturation=color_jitter_strength * 0.5\n",
    "                ),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(*mean_std),\n",
    "                transforms.RandomErasing(p=0.2)\n",
    "            ])\n",
    "            \n",
    "            train_dataset_tuned = FootprintDataset(train_paths, train_labels, train_transform_tuned)\n",
    "            train_loader_tuned = DataLoader(\n",
    "                train_dataset_tuned, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=True, \n",
    "                num_workers=0, \n",
    "                pin_memory=True\n",
    "            )\n",
    "            val_loader_tuned = DataLoader(\n",
    "                val_dataset, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=False, \n",
    "                num_workers=0, \n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            # =================================================================\n",
    "            # CREATE MODEL WITH TUNED DROPOUT\n",
    "            # =================================================================\n",
    "            \n",
    "            model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            \n",
    "            # Replace classifier with dropout\n",
    "            num_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(num_features, 2)\n",
    "            )\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # =================================================================\n",
    "            # CREATE OPTIMIZER\n",
    "            # =================================================================\n",
    "            \n",
    "            if optimizer_name == 'adam':\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            elif optimizer_name == 'adamw':\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            else:  # sgd\n",
    "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # =================================================================\n",
    "            # TRAINING LOOP\n",
    "            # =================================================================\n",
    "            \n",
    "            best_val_acc = 0.0\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Train\n",
    "                model.train()\n",
    "                for inputs, labels in train_loader_tuned:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                # Validate\n",
    "                model.eval()\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader_tuned:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        correct += predicted.eq(labels).sum().item()\n",
    "                        total += labels.size(0)\n",
    "                \n",
    "                val_acc = correct / total\n",
    "                best_val_acc = max(best_val_acc, val_acc)\n",
    "                \n",
    "                # Report intermediate value for pruning\n",
    "                trial.report(val_acc, epoch)\n",
    "                \n",
    "                # Prune trial if not promising\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            return best_val_acc\n",
    "        \n",
    "        return objective\n",
    "    \n",
    "    # =================================================================\n",
    "    # RUN OPTIMIZATION\n",
    "    # =================================================================\n",
    "    \n",
    "    print('='*80)\n",
    "    print('OPTUNA HYPERPARAMETER OPTIMIZATION')\n",
    "    print('='*80)\n",
    "    print('\\nStarting Bayesian optimization...')\n",
    "    print('This will test multiple hyperparameter configurations.')\n",
    "    print('Unpromising trials will be pruned early to save time.\\n')\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',  # Maximize validation accuracy\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)  # Prune after 5 epochs\n",
    "    )\n",
    "    \n",
    "    # Create objective function with data access\n",
    "    objective = create_objective(train_paths, train_labels, val_dataset, device, mean_std)\n",
    "    \n",
    "    # Run optimization - adjust n_trials based on available time\n",
    "    # 30 trials is a good balance between exploration and time\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=30,  # Number of different configurations to try\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # =================================================================\n",
    "    # PRINT RESULTS\n",
    "    # =================================================================\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('OPTIMIZATION COMPLETE')\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f\"\\nBest trial validation accuracy: {study.best_trial.value:.4f}\")\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nTotal trials: {len(study.trials)}\")\n",
    "    pruned_trials = len([t for t in study.trials if t.state == TrialState.PRUNED])\n",
    "    complete_trials = len([t for t in study.trials if t.state == TrialState.COMPLETE])\n",
    "    print(f\"Completed trials: {complete_trials}\")\n",
    "    print(f\"Pruned trials: {pruned_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5b: Hyperparameter Optimization with Optuna\n",
    "\n",
    "This section uses Bayesian optimization to find optimal hyperparameters for the ResNet-18 model.\n",
    "\n",
    "**Hyperparameters being tuned:**\n",
    "- Learning rate (log scale)\n",
    "- Weight decay (log scale)\n",
    "- Optimizer (Adam, SGD, AdamW)\n",
    "- Batch size\n",
    "- Number of epochs\n",
    "- Dropout rate\n",
    "- Augmentation strength (rotation, color jitter)\n",
    "\n",
    "**Why Bayesian Optimization?**\n",
    "- More efficient than grid/random search\n",
    "- Uses previous trial results to guide future choices\n",
    "- Automatically prunes unpromising trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Final Model Evaluation & Explainability\n",
    "\n",
    "This section evaluates the best model with detailed metrics and uses Grad-CAM for explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Best Model\n",
    "if len(train_df) > 0:\n",
    "    best_experiment = max(experiment_results, key=lambda x: x['val_accuracy'])\n",
    "    print('='*80)\n",
    "    print('BEST MODEL')\n",
    "    print('='*80)\n",
    "    print(f\"Name: {best_experiment['name']}\")\n",
    "    print(f\"Validation Accuracy: {best_experiment['val_accuracy']:.4f}\")\n",
    "    print(f\"Validation Loss: {best_experiment['val_loss']:.4f}\")\n",
    "    print('\\nUsing this model for final evaluation...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Metrics on Validation Set\n",
    "if len(train_df) > 0:\n",
    "    from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    final_loss, final_acc, final_preds, final_labels = evaluate(final_model, val_loader, criterion, device)\n",
    "    \n",
    "    print('='*80)\n",
    "    print('DETAILED METRICS')\n",
    "    print('='*80)\n",
    "    print(f'Accuracy: {final_acc:.4f}')\n",
    "    print(f'Loss: {final_loss:.4f}')\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(final_labels, final_preds, average=None)\n",
    "    print('\\nPer-Class Metrics:')\n",
    "    for i in range(2):\n",
    "        print(f'  Class {i} ({\"Female\" if i==0 else \"Male\"}):')\n",
    "        print(f'    Precision: {precision[i]:.4f}')\n",
    "        print(f'    Recall: {recall[i]:.4f}')\n",
    "        print(f'    F1-Score: {f1[i]:.4f}')\n",
    "        print(f'    Support: {support[i]}')\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(final_labels, final_preds, average='macro')\n",
    "    print(f'\\nMacro Averages:')\n",
    "    print(f'  Precision: {macro_p:.4f}')\n",
    "    print(f'  Recall: {macro_r:.4f}')\n",
    "    print(f'  F1-Score: {macro_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "if len(train_df) > 0:\n",
    "    cm = confusion_matrix(final_labels, final_preds)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Female', 'Male'])\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    plt.title('Confusion Matrix - Final Model')\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nConfusion Matrix Analysis:')\n",
    "    print(f'True Negatives (Correctly classified Female): {cm[0,0]}')\n",
    "    print(f'False Positives (Female classified as Male): {cm[0,1]}')\n",
    "    print(f'False Negatives (Male classified as Female): {cm[1,0]}')\n",
    "    print(f'True Positives (Correctly classified Male): {cm[1,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability (XAI) - Grad-CAM\n",
    "\n",
    "**Note:** Grad-CAM visualization helps understand which regions of the footprint image the model focuses on when making predictions.\n",
    "\n",
    "For implementation, you would:\n",
    "1. Select the last convolutional layer\n",
    "2. Register hooks to capture gradients and activations\n",
    "3. Generate heatmaps for sample predictions\n",
    "4. Overlay heatmaps on original images\n",
    "\n",
    "This requires additional implementation based on your model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Kaggle Submission\n",
    "if len(train_df) > 0 and TEST_DIR.exists():\n",
    "    # Load test images\n",
    "    test_paths = sorted(list(TEST_DIR.glob('*.jpg')) + list(TEST_DIR.glob('*.png')))\n",
    "    \n",
    "    if len(test_paths) > 0:\n",
    "        print('='*80)\n",
    "        print('GENERATING KAGGLE SUBMISSION')\n",
    "        print('='*80)\n",
    "        print(f'Found {len(test_paths)} test images')\n",
    "        \n",
    "        # Get test image IDs from filenames\n",
    "        test_ids = [p.stem for p in test_paths]\n",
    "        test_paths_str = [str(p) for p in test_paths]\n",
    "        \n",
    "        # Create test dataset and loader\n",
    "        test_dataset = FootprintDataset(test_paths_str, [0]*len(test_paths_str), val_transform)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=False, \n",
    "            num_workers=0,  # Windows-safe\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Generate predictions\n",
    "        final_model.eval()\n",
    "        test_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in tqdm(test_loader, desc='Predicting test set'):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = final_model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        # Create submission dataframe - Kaggle expects 'filename' column\n",
    "        submission_df = pd.DataFrame({\n",
    "            'filename': test_ids,  # Column name must match Kaggle's expected format\n",
    "            'label': test_predictions\n",
    "        })\n",
    "        \n",
    "        # Save to CSV\n",
    "        submission_df.to_csv('submission.csv', index=False)\n",
    "        \n",
    "        print(f'\\nSubmission file saved: submission.csv')\n",
    "        print(f'Total predictions: {len(submission_df)}')\n",
    "        print('\\nFirst few predictions:')\n",
    "        display(submission_df.head(10))\n",
    "        \n",
    "        # Show prediction distribution\n",
    "        print(f'\\nPrediction distribution:')\n",
    "        print(submission_df['label'].value_counts().sort_index())\n",
    "    else:\n",
    "        print('No test images found in test directory')\n",
    "else:\n",
    "    if len(train_df) == 0:\n",
    "        print('Training data not loaded - cannot generate submission')\n",
    "    else:\n",
    "        print('Test directory not found - cannot generate submission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Conclusion & Reflection\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "**Baseline Performance:**\n",
    "- Simple CNN from scratch\n",
    "- Validation accuracy: [TO FILL after running]\n",
    "\n",
    "**Final Model Performance:**\n",
    "- Best model: [TO FILL after experiments]\n",
    "- Validation accuracy: [TO FILL after running]\n",
    "- Improvement over baseline: [TO FILL]\n",
    "\n",
    "**Key Findings:**\n",
    "- Transfer learning (ResNet/EfficientNet) significantly outperformed from-scratch models\n",
    "- Batch normalization and dropout helped reduce overfitting\n",
    "- Adam optimizer converged faster than SGD\n",
    "- Pre-trained models leveraged ImageNet features effectively\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Small Dataset:** Only 1,573 training images may not capture full diversity\n",
    "2. **Class Imbalance:** May affect model fairness if one class is under-represented\n",
    "3. **Overfitting Risk:** Limited data increases risk of overfitting to training set\n",
    "4. **Demographic Bias:** Dataset may not represent diverse populations\n",
    "5. **Generalization:** Model tested only on validation set from same distribution\n",
    "\n",
    "## Deployment Considerations\n",
    "\n",
    "**Should this model be deployed in forensic investigations?**\n",
    "\n",
    "**Recommendation: NOT as a standalone decision-making tool**\n",
    "\n",
    "**Rationale:**\n",
    "- Model accuracy is good but not perfect (false positives/negatives exist)\n",
    "- Forensic decisions have serious legal and ethical implications\n",
    "- Potential bias in training data could lead to unfair outcomes\n",
    "- Model is a \"black box\" - decisions need to be explainable\n",
    "\n",
    "**Acceptable Use:**\n",
    "- As an **investigative support tool** alongside human expert analysis\n",
    "- Requires human oversight and verification\n",
    "- Should be one piece of evidence, not the sole determinant\n",
    "- Needs extensive validation on diverse, real-world data\n",
    "- Requires transparency about limitations and error rates\n",
    "\n",
    "## Future Work\n",
    "\n",
    "1. **More Data:** Collect larger, more diverse dataset\n",
    "2. **Cross-Validation:** Implement k-fold CV for robust evaluation\n",
    "3. **Advanced Augmentation:** Test stronger data augmentation techniques\n",
    "4. **Ensemble Methods:** Combine multiple models for better predictions\n",
    "5. **Better XAI:** Implement comprehensive Grad-CAM and SHAP analysis\n",
    "6. **Calibration:** Ensure predicted probabilities are well-calibrated\n",
    "7. **External Validation:** Test on completely separate datasets\n",
    "8. **Bias Analysis:** Systematic analysis of potential biases\n",
    "9. **Uncertainty Quantification:** Provide confidence intervals for predictions\n",
    "10. **Real-world Testing:** Pilot study with forensic experts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
