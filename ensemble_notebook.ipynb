{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Architecture Ensemble for Footprint Classification\n",
    "\n",
    "This notebook trains an ensemble of 3 different architectures (ResNet-34, EfficientNet-B0, ConvNeXt-Tiny) and generates a Kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torchvision.transforms.functional as TF\n",
    "import timm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Configuration:\n",
      "  Model: ResNet-34 x 3 (different seeds)\n",
      "  Seeds: [42, 123, 456]\n",
      "  Image size: 224\n",
      "  Batch size: 128\n",
      "  Epochs: 25\n",
      "  Learning rate: 0.0005\n",
      "  Dropout: 0.5\n",
      "  Early stopping patience: 7\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "# Ensemble of 3 ResNet-34 models with different seeds for diversity\n",
    "MODEL_CONFIGS = [\n",
    "    {'name': 'ResNet-34-seed42', 'type': 'resnet34', 'seed': 42},\n",
    "    {'name': 'ResNet-34-seed123', 'type': 'resnet34', 'seed': 123},\n",
    "    {'name': 'ResNet-34-seed456', 'type': 'resnet34', 'seed': 456},\n",
    "]\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25\n",
    "LR = 0.0005\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT = 0.5  # Changed from 0.3 to match main notebook\n",
    "EARLY_STOP_PATIENCE = 7\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = Path('./data')\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "TEST_DIR = DATA_DIR / 'test'\n",
    "\n",
    "print(f'Ensemble Configuration:')\n",
    "print(f'  Model: ResNet-34 x 3 (different seeds)')\n",
    "print(f'  Seeds: {[m[\"seed\"] for m in MODEL_CONFIGS]}')\n",
    "print(f'  Image size: {IMG_SIZE}')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Epochs: {EPOCHS}')\n",
    "print(f'  Learning rate: {LR}')\n",
    "print(f'  Dropout: {DROPOUT}')\n",
    "print(f'  Early stopping patience: {EARLY_STOP_PATIENCE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image mode: RGB\n",
      "Total training images: 1573\n",
      "Class distribution: 0(Female)=845, 1(Male)=728\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load image paths and labels from directory structure\"\"\"\n",
    "    paths, labels = [], []\n",
    "    # Folders are named '0' (Female) and '1' (Male)\n",
    "    for class_idx in [0, 1]:\n",
    "        class_dir = data_dir / str(class_idx)\n",
    "        if class_dir.exists():\n",
    "            for img_path in class_dir.glob('*'):\n",
    "                if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                    paths.append(img_path)\n",
    "                    labels.append(class_idx)\n",
    "    return paths, labels\n",
    "\n",
    "# Check if grayscale\n",
    "sample_img = Image.open(list(TRAIN_DIR.glob('*/*'))[0])\n",
    "IS_GRAYSCALE = sample_img.mode == 'L'\n",
    "INPUT_CHANNELS = 1 if IS_GRAYSCALE else 3\n",
    "print(f'Image mode: {\"Grayscale\" if IS_GRAYSCALE else \"RGB\"}')\n",
    "\n",
    "# Normalization\n",
    "mean_std = ([0.5], [0.5]) if IS_GRAYSCALE else ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# Training transforms - simpler augmentation (matches main notebook's best config)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Only HorizontalFlip - best from main notebook\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std),\n",
    "])\n",
    "\n",
    "# Validation/test transforms\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "class FootprintDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('L' if IS_GRAYSCALE else 'RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "# Load and split data\n",
    "train_paths, train_labels = load_data(TRAIN_DIR)\n",
    "print(f'Total training images: {len(train_paths)}')\n",
    "print(f'Class distribution: 0(Female)={sum(1 for l in train_labels if l==0)}, 1(Male)={sum(1 for l in train_labels if l==1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model creation:\n",
      "  ResNet-34-seed42: 21,285,698 parameters\n",
      "  ResNet-34-seed123: 21,285,698 parameters\n",
      "  ResNet-34-seed456: 21,285,698 parameters\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Factory\n",
    "\n",
    "def create_model(model_type):\n",
    "    \"\"\"Create a model based on the specified type\"\"\"\n",
    "    \n",
    "    if model_type == 'resnet34':\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        if INPUT_CHANNELS == 1:\n",
    "            model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(model.fc.in_features, 2)\n",
    "        )\n",
    "        \n",
    "    elif model_type == 'efficientnet_b0':\n",
    "        model = timm.create_model('efficientnet_b0', pretrained=True, \n",
    "                                   num_classes=2, in_chans=INPUT_CHANNELS, drop_rate=DROPOUT)\n",
    "        \n",
    "    elif model_type == 'mobilenetv3':\n",
    "        model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "        if INPUT_CHANNELS == 1:\n",
    "            model.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, 2)\n",
    "        \n",
    "    elif model_type == 'convnext_tiny':\n",
    "        model = timm.create_model('convnext_tiny', pretrained=True, \n",
    "                                   num_classes=2, in_chans=INPUT_CHANNELS, drop_rate=DROPOUT)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model type: {model_type}')\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Test model creation for each architecture\n",
    "print('Testing model creation:')\n",
    "for config in MODEL_CONFIGS:\n",
    "    test_model = create_model(config['type'])\n",
    "    params = sum(p.numel() for p in test_model.parameters())\n",
    "    print(f\"  {config['name']}: {params:,} parameters\")\n",
    "    del test_model\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Function\n",
    "\n",
    "def train_single_model(model_config, train_paths, train_labels):\n",
    "    \"\"\"Train a single model with the given configuration\"\"\"\n",
    "    model_name = model_config['name']\n",
    "    model_type = model_config['type']\n",
    "    seed = model_config['seed']\n",
    "    \n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {model_name} (seed={seed})')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Split data with this seed\n",
    "    t_paths, v_paths, t_labels, v_labels = train_test_split(\n",
    "        train_paths, train_labels, test_size=VAL_SPLIT, \n",
    "        stratify=train_labels, random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = FootprintDataset(t_paths, t_labels, train_transform)\n",
    "    val_dataset = FootprintDataset(v_paths, v_labels, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(model_type)\n",
    "    \n",
    "    # Class weights for imbalanced data\n",
    "    n_female = sum(1 for l in t_labels if l == 0)\n",
    "    n_male = sum(1 for l in t_labels if l == 1)\n",
    "    total = n_female + n_male\n",
    "    class_weights = torch.tensor([\n",
    "        total / (2 * n_female),\n",
    "        total / (2 * n_male)\n",
    "    ]).to(device)\n",
    "    \n",
    "    # Loss, optimizer, scheduler (StepLR - matches main notebook)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # StepLR like main notebook\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model and check early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{EPOCHS}] - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "            print(f'Early stopping at epoch {epoch+1} (no improvement for {EARLY_STOP_PATIENCE} epochs)')\n",
    "            break\n",
    "    \n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f'Best validation accuracy: {best_val_acc:.4f}')\n",
    "    \n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble of 3 different architectures...\n",
      "Models: ['ResNet-34-seed42', 'ResNet-34-seed123', 'ResNet-34-seed456']\n",
      "\n",
      "[Model 1/3]\n",
      "\n",
      "============================================================\n",
      "Training ResNet-34-seed42 (seed=42)\n",
      "============================================================\n",
      "Epoch [1/25] - Train Acc: 0.6661, Val Acc: 0.5079\n",
      "Epoch [5/25] - Train Acc: 0.9245, Val Acc: 0.6635\n",
      "Epoch [10/25] - Train Acc: 0.9897, Val Acc: 0.9016\n",
      "Epoch [15/25] - Train Acc: 0.9992, Val Acc: 0.8921\n",
      "Early stopping at epoch 17 (no improvement for 7 epochs)\n",
      "Best validation accuracy: 0.9016\n",
      "Saved: ensemble_ResNet_34_seed42.pth\n",
      "\n",
      "[Model 2/3]\n",
      "\n",
      "============================================================\n",
      "Training ResNet-34-seed123 (seed=123)\n",
      "============================================================\n",
      "Epoch [1/25] - Train Acc: 0.6479, Val Acc: 0.5683\n",
      "Epoch [5/25] - Train Acc: 0.9237, Val Acc: 0.7556\n",
      "Epoch [10/25] - Train Acc: 0.9905, Val Acc: 0.8698\n",
      "Epoch [15/25] - Train Acc: 0.9992, Val Acc: 0.8825\n",
      "Epoch [20/25] - Train Acc: 1.0000, Val Acc: 0.8730\n",
      "Early stopping at epoch 21 (no improvement for 7 epochs)\n",
      "Best validation accuracy: 0.8889\n",
      "Saved: ensemble_ResNet_34_seed123.pth\n",
      "\n",
      "[Model 3/3]\n",
      "\n",
      "============================================================\n",
      "Training ResNet-34-seed456 (seed=456)\n",
      "============================================================\n",
      "Epoch [1/25] - Train Acc: 0.6407, Val Acc: 0.6349\n",
      "Epoch [5/25] - Train Acc: 0.9141, Val Acc: 0.8476\n",
      "Epoch [10/25] - Train Acc: 0.9833, Val Acc: 0.8222\n",
      "Early stopping at epoch 12 (no improvement for 7 epochs)\n",
      "Best validation accuracy: 0.8476\n",
      "Saved: ensemble_ResNet_34_seed456.pth\n",
      "\n",
      "============================================================\n",
      "ENSEMBLE TRAINING COMPLETE\n",
      "============================================================\n",
      "  ResNet-34-seed42: 0.9016\n",
      "  ResNet-34-seed123: 0.8889\n",
      "  ResNet-34-seed456: 0.8476\n",
      "\n",
      "Mean accuracy: 0.8794\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train Ensemble\n",
    "\n",
    "print(f'Training ensemble of {len(MODEL_CONFIGS)} different architectures...')\n",
    "print(f'Models: {[m[\"name\"] for m in MODEL_CONFIGS]}')\n",
    "\n",
    "ensemble_models = []\n",
    "ensemble_accuracies = []\n",
    "ensemble_names = []\n",
    "\n",
    "for i, config in enumerate(MODEL_CONFIGS):\n",
    "    print(f'\\n[Model {i+1}/{len(MODEL_CONFIGS)}]')\n",
    "    \n",
    "    model, val_acc = train_single_model(config, train_paths, train_labels)\n",
    "    ensemble_models.append(model)\n",
    "    ensemble_accuracies.append(val_acc)\n",
    "    ensemble_names.append(config['name'])\n",
    "    \n",
    "    # Save checkpoint\n",
    "    safe_name = config['name'].replace('-', '_').replace(' ', '_')\n",
    "    torch.save(model.state_dict(), f'ensemble_{safe_name}.pth')\n",
    "    print(f'Saved: ensemble_{safe_name}.pth')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('ENSEMBLE TRAINING COMPLETE')\n",
    "print(f'{\"=\"*60}')\n",
    "for name, acc in zip(ensemble_names, ensemble_accuracies):\n",
    "    print(f'  {name}: {acc:.4f}')\n",
    "print(f'\\nMean accuracy: {np.mean(ensemble_accuracies):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: TTA Prediction Functions (HorizontalFlip only - matches main notebook)\n\nclass TestDataset(Dataset):\n    \"\"\"Dataset for test images (no labels)\"\"\"\n    def __init__(self, paths, transform):\n        self.paths = paths\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.paths[idx]).convert('L' if IS_GRAYSCALE else 'RGB')\n        img_transformed = self.transform(img)\n        return img_transformed, self.paths[idx].stem  # .stem = filename WITHOUT extension\n\ndef apply_tta(model, image_tensor):\n    \"\"\"Apply test-time augmentation (horizontal flip only)\n    \n    Returns averaged probabilities across:\n    - Original image\n    - Horizontal flip\n    \n    Note: Rotations removed because footprints are NOT rotationally invariant\n    \"\"\"\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        # Original\n        pred = F.softmax(model(image_tensor), dim=1)\n        predictions.append(pred)\n        \n        # Horizontal flip only\n        flipped = torch.flip(image_tensor, dims=[3])\n        pred = F.softmax(model(flipped), dim=1)\n        predictions.append(pred)\n    \n    # Average predictions\n    return torch.stack(predictions).mean(dim=0)\n\ndef ensemble_predict_with_tta(models, image_tensor):\n    \"\"\"Get ensemble prediction with TTA for each model\"\"\"\n    all_predictions = []\n    \n    for model in models:\n        model_pred = apply_tta(model, image_tensor)\n        all_predictions.append(model_pred)\n    \n    # Average across all models\n    ensemble_pred = torch.stack(all_predictions).mean(dim=0)\n    return ensemble_pred\n\nprint('TTA functions defined:')\nprint('  - 2 augmentations per model (original + horizontal flip)')\nprint(f'  - {len(MODEL_CONFIGS)} models in ensemble')\nprint(f'  - Total predictions averaged: {2 * len(MODEL_CONFIGS)}')"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1055 test images\n",
      "\n",
      "Generating predictions with ensemble + TTA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1055/1055 [01:17<00:00, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1055 predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Generate Submission\n",
    "\n",
    "# Load test images\n",
    "test_paths = sorted(list(TEST_DIR.glob('*.png')) + list(TEST_DIR.glob('*.jpg')) + list(TEST_DIR.glob('*.jpeg')))\n",
    "print(f'Found {len(test_paths)} test images')\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset = TestDataset(test_paths, val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Set all models to eval mode\n",
    "for model in ensemble_models:\n",
    "    model.eval()\n",
    "\n",
    "# Generate predictions\n",
    "filenames = []\n",
    "predictions = []\n",
    "\n",
    "print('\\nGenerating predictions with ensemble + TTA...')\n",
    "for image, filename in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Get ensemble prediction with TTA\n",
    "    probs = ensemble_predict_with_tta(ensemble_models, image)\n",
    "    pred_class = probs.argmax(dim=1).item()\n",
    "    \n",
    "    filenames.append(filename[0])\n",
    "    predictions.append(pred_class)\n",
    "\n",
    "print(f'Generated {len(predictions)} predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n",
      "\n",
      "Total predictions: 1055\n",
      "\n",
      "Class distribution (0=Female, 1=Male):\n",
      "label\n",
      "0    632\n",
      "1    423\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 10 rows:\n",
      "   filename  label\n",
      "0  img_0003      1\n",
      "1  img_0004      1\n",
      "2  img_0005      0\n",
      "3  img_0006      0\n",
      "4  img_0009      0\n",
      "5  img_0010      0\n",
      "6  img_0011      0\n",
      "7  img_0018      0\n",
      "8  img_0019      0\n",
      "9  img_0022      1\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save Submission\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'label': predictions\n",
    "})\n",
    "\n",
    "# Sort by filename for consistency\n",
    "submission_df = submission_df.sort_values('filename').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission saved to submission.csv')\n",
    "print(f'\\nTotal predictions: {len(submission_df)}')\n",
    "print(f'\\nClass distribution (0=Female, 1=Male):')\n",
    "print(submission_df['label'].value_counts())\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}